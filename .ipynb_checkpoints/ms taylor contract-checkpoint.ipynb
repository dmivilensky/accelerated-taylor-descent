{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "p = 20000\n",
    "m = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum_{i = 1}^n \\lambda_i = 1,\\quad [e_i]_j \\sim \\mathcal{U}(1, 2)$ for all $j \\in [1, n]$\n",
    "\n",
    "$G^2 = \\sum_{i = 1}^n \\lambda_i e_i e_i^\\top$\n",
    "\n",
    "$L_i = G^2_{i i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = np.random.random(n)\n",
    "lambda_ /= lambda_.sum()\n",
    "\n",
    "e = np.matrix(np.random.random(size=(n, n)) + 1)\n",
    "\n",
    "G_wave = np.array(np.multiply(e, lambda_).dot(e.T))\n",
    "\n",
    "Li = np.diag(G_wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_G_correctness(G):\n",
    "    \"\"\"\n",
    "    Test for matrix G generation\n",
    "    \n",
    "    param: G             generated matrix\n",
    "    :return: correctness matrix correctness boolean flag\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.linalg.norm(G - G.T) < 1e-10 and \\\n",
    "           np.linalg.eig(G)[0].min() >= 0 and \\\n",
    "           G.min() >= 0 and \\\n",
    "           G.min() <= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_G_correctness(G_wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A_{i j} = 0$ or $A_{i j} \\sim \\mathcal{U}(-1, 1)$\n",
    "\n",
    "$\\mathbb{P}[A_{i j} = 0] = 0.99$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = 0.001\n",
    "\n",
    "A = np.zeros(shape=(p, n))\n",
    "A[\n",
    "    np.random.randint(p, size=int(sparsity*(p*n))), \n",
    "    np.random.randint(n, size=int(sparsity*(p*n)))\n",
    "] = np.random.random(int(sparsity*(p*n))) * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = h(x) + g(x)$\n",
    "\n",
    "$h(x) = \\log\\left(\\sum_{k=1}^p \\exp\\left(\\langle A_k, x \\rangle\\right)\\right)$\n",
    "\n",
    "$g(x) = \\frac{1}{2} \\|G x\\|_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x, dot_product=None):\n",
    "    \"\"\"\n",
    "    Log sum exp functional term\n",
    "    \n",
    "    param: x        variable\n",
    "    :return: result functional term value\n",
    "    \"\"\"\n",
    "    \n",
    "    if dot_product is not None:\n",
    "        t = dot_product\n",
    "    else:\n",
    "        t = np.dot(A, x)\n",
    "    u = t.max()\n",
    "    t -= u\n",
    "    # Log sum exp trick\n",
    "    return u + np.log(np.exp(t).sum())\n",
    "\n",
    "g = lambda x: min(0.5 * np.dot(x, np.dot(G_wave, x)), 1e142)\n",
    "\n",
    "\n",
    "def f(x, dot_product=None): \n",
    "    return g(x) + h(x, dot_product=dot_product)\n",
    "\n",
    "\n",
    "def grad_h(x):\n",
    "    \"\"\"\n",
    "    Gradient of the log sum exp functional term\n",
    "    \n",
    "    param: x        variable\n",
    "    :return: result gradient of the functional term value\n",
    "    \"\"\"\n",
    "    \n",
    "    s = np.dot(A, x)\n",
    "    b = s.max()\n",
    "    z = np.exp(s - b)\n",
    "    # Exp-normalize trick\n",
    "    return np.dot(A.T, z) / np.dot(np.ones(p), z)\n",
    "\n",
    "\n",
    "def grad_h_stoch(x, i, dot_product=None):\n",
    "    \"\"\"\n",
    "    Stochastic gradient of the log sum exp functional term\n",
    "    \n",
    "    param: x        variable\n",
    "    param: i        component\n",
    "    :return: result i-th component of the gradient of the functional term value\n",
    "    \"\"\"\n",
    "    \n",
    "    if dot_product is not None:\n",
    "        s = dot_product\n",
    "    else:\n",
    "        s = np.dot(A, x)\n",
    "    b = s.max()\n",
    "    z = np.exp(s - b)\n",
    "    # Exp-normalize trick\n",
    "    return np.dot(A.T[i], z) / np.dot(np.ones(p), z)\n",
    "\n",
    "grad_g = lambda x: np.dot(G_wave, x)\n",
    "grad_g_stoch = lambda x, i: np.dot(G_wave[i], x)\n",
    "\n",
    "grad_f = lambda x: grad_g(x) + grad_h(x)\n",
    "\n",
    "\n",
    "def grad_f_stoch(x, i, dot_product=None): \n",
    "    return grad_g_stoch(x, i) + grad_h_stoch(x, i, dot_product=dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\varphi_{\\zeta^k, \\zeta^0}(\\zeta) = \\langle \\nabla h(\\zeta^k), \\zeta - \\zeta^k \\rangle + g(\\zeta) + \\frac{L}{2} \\|\\zeta - \\zeta^0\\|_2^2 + \\frac{L_h}{2} \\|\\zeta - \\zeta^k\\|_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(zeta, zeta_k, zeta_0, L_h, L, h_grad=None):\n",
    "    \"\"\"\n",
    "    Proximal phi function\n",
    "    \n",
    "    param: zeta   variable\n",
    "    param: zeta_k variable value at the certain iteration of composite method\n",
    "    param: zeta_0 initial variable value for the composite method\n",
    "    param: L_h, L Lipschitz constants\n",
    "    \n",
    "    :return: result \n",
    "    \"\"\"\n",
    "    if h_grad is None:\n",
    "        h_grad = grad_h(zeta_k)\n",
    "    \n",
    "    return np.dot(h_grad, zeta - zeta_k) + g(zeta) + \\\n",
    "           0.5 * L_h * np.linalg.norm(zeta - zeta_k)**2 + \\\n",
    "           0.5 * L * np.linalg.norm(zeta - zeta_0)**2\n",
    "\n",
    "\n",
    "def grad_phi(zeta, zeta_k, zeta_0, L_h, L, h_grad=None):\n",
    "    \"\"\"\n",
    "    Gradient of the proximal phi function\n",
    "    \n",
    "    param: zeta   variable\n",
    "    param: zeta_k variable value at the certain iteration of composite method\n",
    "    param: zeta_0 initial variable value for the composite method\n",
    "    param: L_h, L Lipschitz constants\n",
    "    \n",
    "    :return: result \n",
    "    \"\"\"\n",
    "    if h_grad is None:\n",
    "        h_grad = grad_h(zeta_k)\n",
    "    \n",
    "    return grad_h(zeta_k) + grad_g(zeta) + \\\n",
    "           L_h * (zeta - zeta_k) + \\\n",
    "           L * (zeta - zeta_0)\n",
    "\n",
    "\n",
    "def grad_phi_stoch(zeta, zeta_k, zeta_0, L_h, L, i, h_grad=None):\n",
    "    \"\"\"\n",
    "    Gradient of the proximal phi function\n",
    "    \n",
    "    param: zeta   variable\n",
    "    param: zeta_k variable value at the certain iteration of composite method\n",
    "    param: zeta_0 initial variable value for the composite method\n",
    "    param: L_h, L Lipschitz constants\n",
    "    param: i      component\n",
    "    \n",
    "    :return: result \n",
    "    \"\"\"\n",
    "    if h_grad is None:\n",
    "        h_grad_stoch = grad_h_stoch(zeta_k, i)\n",
    "    else:\n",
    "        h_grad_stoch = h_grad[i]\n",
    "    \n",
    "    return h_grad_stoch + grad_g_stoch(zeta, i) + \\\n",
    "           L_h * (zeta[i] - zeta_k[i]) + \\\n",
    "           L * (zeta[i] - zeta_0[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\psi_{x, L_h}(y) = h(x) + \\langle \\nabla h(x), y - x \\rangle + L_h \\|y - x\\|_2^2 + g(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(x, y, L_h, h_val=None, h_grad=None):\n",
    "    if h_grad is None:\n",
    "        h_grad = grad_h(x)\n",
    "        \n",
    "    if h_val is None:\n",
    "        h_val = h(x)\n",
    "    \n",
    "    return h_val + h_grad.dot(y - x) + L_h * np.linalg.norm(y - x)**2 + g(y)\n",
    "\n",
    "\n",
    "def grad_psi_stoch(x, y, L_h, i, h_grad=None):\n",
    "    if h_grad is None:\n",
    "        h_grad_stoch = grad_h_stoch(x, i)\n",
    "    else:\n",
    "        h_grad_stoch = h_grad[i]\n",
    "    \n",
    "    return h_grad_stoch + 2 * L_h * (y[i] - x[i]) + grad_g_stoch(y, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_{x^k, v^k, \\gamma, A_k, A_{k+1}, a_{k+1}}(x) = A_{k+1} h\\left(\\frac{a_{k+1} x + A_k x^k}{A_{k+1}}\\right) + a_{k+1} g(x) + \\gamma \\beta(v^k, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = lambda x, y: 0.5 * np.linalg.norm(y - x) ** 2\n",
    "\n",
    "\n",
    "def H(x, x_k, v_k, gamma, A, A_new, a):\n",
    "    return A_new * h((a * x + A * x_k) / A_new) + a * g(x) + gamma * beta(v_k, x)\n",
    "\n",
    "\n",
    "def grad_H(x, x_k, v_k, gamma, A, A_new, a):\n",
    "    return a * grad_h((a * x + A * x_k) / A_new) + a * grad_g(x) + gamma * (x - v_k)\n",
    "\n",
    "\n",
    "def grad_H_stoch(x, x_k, v_k, gamma, A, A_new, a, i):\n",
    "    return a * grad_h_stoch((a * x + A * x_k) / A_new, i) + a * grad_g_stoch(x, i) + gamma * (x[i] - v_k[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov(func, grad_func_stoch, x0, A_matrix, T=100, L_add=0, L_mult=1, restart=None, beta=1/2, return_f=True, inner=False, time_scale=False, verbose=False, print_grads=False, grad_func=None):\n",
    "    \"\"\"\n",
    "    Nesterov's Fast Coordinate Descent method\n",
    "    paper: Nesterov and Stich\n",
    "           \"Efficiency of the Accelerated Coordinate Descent Method on Structured Optimization Problems\"\n",
    "           (presentation http://www.mathnet.ru:8080/PresentFiles/11909/7_nesterov.pdf , page 6)\n",
    "    \n",
    "    param: func             objective functional\n",
    "    param: grad_func_stoch  stochastic gradient of the objective functional\n",
    "    param: x0               starting point\n",
    "    param: T                maximum number of iterations\n",
    "    param: L_add            summand for all directional Lipschitz constants\n",
    "    param: restart          count of iterations between two consequent restarts\n",
    "    param: beta             parameter of randomizer normalization constants\n",
    "    param: return_f         if true, then procedure returns the history of functional values\n",
    "    param: time_scale       if true, then procedure returns the times of all iterations\n",
    "    param: verbose          if true, then procedure logs debug information\n",
    "    \n",
    "    :return: x                     result value\n",
    "    :return: funcs (if return_f)   history of functional values    \n",
    "    :return: times (if return_f)   times for all iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x0.copy()\n",
    "    v = x0.copy()\n",
    "    \n",
    "    A = 0\n",
    "    S = ((Li*L_mult+L_add)**beta).sum()\n",
    "    \n",
    "    funcs = []\n",
    "    times = []\n",
    "    \n",
    "    grad_h_count = [0]\n",
    "    grad_g_count = [0]\n",
    "    \n",
    "    time_start = time.time()\n",
    "    \n",
    "    eye = np.eye(n)\n",
    "    \n",
    "    for k in range(T):\n",
    "        i = int(np.random.choice(np.linspace(0, n-1, n), \n",
    "                                 p=(Li*L_mult+L_add)**beta/S))\n",
    "        a = np.roots([S**2, -1, -A]).max()\n",
    "        A = A + a\n",
    "        alpha = a / A\n",
    "        \n",
    "        y = (1 - alpha) * x + alpha * v\n",
    "        \n",
    "        stoch_grad = grad_func_stoch(y, i)\n",
    "        \n",
    "        if not inner:\n",
    "            grad_h_count.append(grad_h_count[-1] + 1)\n",
    "            grad_g_count.append(grad_g_count[-1] + 1)\n",
    "        else:\n",
    "            grad_h_count.append(grad_h_count[-1])\n",
    "            grad_g_count.append(grad_g_count[-1] + 1)\n",
    "        \n",
    "        gamma = - (1 / (Li[i]*L_mult+L_add)) * stoch_grad \n",
    "        zeta = - (a * S) / ((Li[i]*L_mult+L_add)**beta) * stoch_grad\n",
    "        \n",
    "        x = y + gamma * eye[i]\n",
    "        \n",
    "        v = v + zeta * eye[i]\n",
    "        \n",
    "        if restart is not None and k % restart == 0:\n",
    "            v = x.copy()\n",
    "            \n",
    "        if time_scale:\n",
    "            times.append(time.time())\n",
    "        \n",
    "        funcs.append(f(x))\n",
    "        if verbose:\n",
    "            print(f(x))\n",
    "            \n",
    "        if print_grads:\n",
    "            print(np.linalg.norm(grad_func(x)))\n",
    "    \n",
    "    if return_f:\n",
    "        return x, np.array(funcs), np.array(grad_h_count), np.array(grad_g_count), times\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_h = \\max_{k = 1,...,n} \\|A^{<k>}\\|^2_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lh = max([np.linalg.norm(A[:, k])**2 for k in range(n)])\n",
    "L = Lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.random.random(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_composite(zeta0, L, Lh, A_matrix, time_scale):\n",
    "    \"\"\"\n",
    "    Composite Gradient method for optimizing proximal phi function\n",
    "    \n",
    "    param: zeta0      starting point\n",
    "    param: T          maximum number of iterations\n",
    "    param: L, Lh      Lipschitz constants\n",
    "    param: time_scale if true, then procedure returns the times of all iterations\n",
    "    \n",
    "    :return: zeta         result value\n",
    "    :return: history_comp history of functional values of the M_inn method (Nesterov FCD)\n",
    "    :return: times        times for all iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    zeta = zeta0.copy()\n",
    "    history_comp = []\n",
    "    \n",
    "    times = []\n",
    "    \n",
    "    grad_h_count = []\n",
    "    grad_g_count = []\n",
    "    \n",
    "    for i in range(1):\n",
    "        zeta, history_inner, grad_h_count_inner, grad_g_count_inner, times_inner = nesterov(\n",
    "            lambda zeta_var: phi(zeta_var, zeta, zeta0, Lh, L),#, h_grad=h_grad),\n",
    "            lambda zeta_var, i: grad_phi_stoch(zeta_var, zeta, zeta0, Lh, L, i),#, h_grad=h_grad),\n",
    "            zeta.copy(),\n",
    "            A_matrix,\n",
    "            T=50,\n",
    "            L_add=2*Lh,\n",
    "            restart=None,\n",
    "            inner=True,\n",
    "            return_f=True,\n",
    "            time_scale=time_scale\n",
    "        )\n",
    "        \n",
    "        grad_h_count = (1 + grad_h_count_inner).tolist()[1:]\n",
    "        grad_g_count = (grad_g_count_inner).tolist()[1:]\n",
    "            \n",
    "        times += times_inner\n",
    "        history_comp += history_inner.tolist()\n",
    "        \n",
    "        if np.linalg.norm(grad_f(zeta) + L*(zeta - zeta0)) <= L / 2 * np.linalg.norm(zeta - zeta0):\n",
    "            break\n",
    "    \n",
    "    return zeta, history_comp, np.array(grad_h_count), np.array(grad_g_count), times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_acc_prox_method(x0, L, Lh, A_matrix, T, time_scale, verbose=False):\n",
    "    \"\"\"\n",
    "    Monteiro–Svaiter algorithm\n",
    "    \n",
    "    param: x0         starting point\n",
    "    param: T          maximum number of iterations\n",
    "    param: L, Lh      Lipschitz constants\n",
    "    param: time_scale if true, then procedure returns the times of all iterations\n",
    "    param: verbose    if true, then procedure logs debug information\n",
    "    \n",
    "    :return: zeta         result value\n",
    "    :return: history_comp history of functional values of the M_inn method (Nesterov FCD)\n",
    "    :return: times        times for all iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x0.copy()\n",
    "    y = x0.copy()\n",
    "    z = x0.copy()\n",
    "\n",
    "    history = []\n",
    "    times = []\n",
    "    \n",
    "    grad_h_count = [0]\n",
    "    grad_g_count = [0]\n",
    "    \n",
    "    Ak = 0\n",
    "    for it in range(T):\n",
    "        ak = (1/L + np.sqrt(1/L**2 + 4 * Ak / L)) / 2\n",
    "        Ak_new = Ak + ak\n",
    "        \n",
    "        x = (Ak / Ak_new) * y + (ak / Ak_new) * z\n",
    "        y, history_comp, grad_h_count_comp, grad_g_count_comp, times_comp = grad_composite(\n",
    "            x, Lh, L, A_matrix, time_scale=time_scale\n",
    "        )\n",
    "        \n",
    "        if len(grad_h_count) == 1:\n",
    "            grad_h_count = (grad_h_count[-1] + grad_h_count_comp).tolist()\n",
    "            grad_g_count = (grad_g_count[-1] + grad_g_count_comp).tolist()\n",
    "        else:\n",
    "            grad_h_count += (grad_h_count[-1] + grad_h_count_comp).tolist()\n",
    "            grad_g_count += (grad_g_count[-1] + grad_g_count_comp).tolist()\n",
    "        \n",
    "        z = z - ak*0.04*grad_f(y)\n",
    "        \n",
    "        grad_h_count[-1] = grad_h_count[-1] + 2\n",
    "        grad_g_count[-1] = grad_g_count[-1] + 2\n",
    "        \n",
    "        Ak = Ak_new\n",
    "        \n",
    "        times += times_comp\n",
    "        history += history_comp\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"y\", history_comp[-1])\n",
    "            print(\"--\", ak)\n",
    "        \n",
    "    return y, np.array(history), grad_h_count, grad_g_count, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_acc_prox_method(x0, L, Lh, A_matrix, T_inner, T, time_scale, verbose=False):\n",
    "    \"\"\"\n",
    "    Taylor Descent\n",
    "    \n",
    "    param: x0         starting point\n",
    "    param: T          maximum number of iterations\n",
    "    param: L, Lh      Lipschitz constants\n",
    "    param: time_scale if true, then procedure returns the times of all iterations\n",
    "    param: verbose    if true, then procedure logs debug information\n",
    "    \n",
    "    :return: zeta         result value\n",
    "    :return: history_comp history of functional values of the M_inn method (Nesterov FCD)\n",
    "    :return: times        times for all iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x0.copy()\n",
    "    y = x0.copy()\n",
    "\n",
    "    history = []\n",
    "    times = []\n",
    "    \n",
    "    grad_h_count = [0]\n",
    "    grad_g_count = [0]\n",
    "    \n",
    "    lambda_k = 1 / (2*L)\n",
    "    Ak = 0\n",
    "    for it in range(T):\n",
    "        ak = (lambda_k + np.sqrt(lambda_k**2 + 4*Ak*lambda_k)) / 2\n",
    "        Ak_new = Ak + ak\n",
    "        \n",
    "        x_wave = (Ak/Ak_new) * y + (ak/Ak_new) * x\n",
    "        \n",
    "        h_val = h(x_wave)\n",
    "        \n",
    "        y, history_comp, grad_h_count_comp, grad_g_count_comp, times_comp = nesterov(\n",
    "            lambda y_var: psi(x_wave.copy(), y_var, Lh, h_val=h_val),#, h_grad=h_grad),\n",
    "            lambda y_var, i: grad_psi_stoch(x_wave.copy(), y_var, Lh, i),#, h_grad=h_grad),\n",
    "            x_wave.copy(),\n",
    "            A_matrix,\n",
    "            T=T_inner,\n",
    "            L_add=3*Lh,\n",
    "            restart=None,\n",
    "            inner=True,\n",
    "            return_f=True,\n",
    "            time_scale=time_scale\n",
    "        )\n",
    "        \n",
    "        grad_h_count += (grad_h_count[-1] + 1 + grad_h_count_comp).tolist()[1:]\n",
    "        grad_g_count += (grad_g_count[-1] + grad_g_count_comp).tolist()[1:]\n",
    "        \n",
    "        x = x - ak*grad_f(y)\n",
    "        \n",
    "        grad_h_count[-1] = grad_h_count[-1] + 2.2\n",
    "        grad_g_count[-1] = grad_g_count[-1] + 2.2\n",
    "        \n",
    "        Ak = Ak_new\n",
    "        \n",
    "        times += times_comp\n",
    "        history += history_comp.tolist()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"y\", history_comp[-1])\n",
    "            print(\"--\", ak)\n",
    "        \n",
    "    return y, np.array(history), grad_h_count, grad_g_count, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgm(func, grad_func, x0, T=1000, L_const=0, return_f=True, time_scale=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Fast Gradient method\n",
    "    \n",
    "    param: func             objective functional\n",
    "    param: grad_func        gradient of the objective functional\n",
    "    param: x0               starting point\n",
    "    param: T                maximum number of iterations\n",
    "    param: L_const          Lipschitz constant\n",
    "    param: return_f         if true, then procedure returns the history of functional values\n",
    "    param: time_scale       if true, then procedure returns the times of all iterations\n",
    "    param: verbose          if true, then procedure logs debug information\n",
    "    \n",
    "    :return: x                     result value\n",
    "    :return: funcs (if return_f)   history of functional values    \n",
    "    :return: times (if return_f)   times for all iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x0.copy()\n",
    "    v = x0.copy()\n",
    "    \n",
    "    A = 0\n",
    "    S = L_const / 2\n",
    "    \n",
    "    funcs = []\n",
    "    times = []\n",
    "    \n",
    "    grad_h_count = [0]\n",
    "    grad_g_count = [0]\n",
    "    \n",
    "    for k in range(T):\n",
    "        a = np.roots([S, -1, -A]).max()\n",
    "        A = A + a\n",
    "        alpha = a / A\n",
    "        \n",
    "        y = (1 - alpha) * x + alpha * v\n",
    "        x = y - (1 / L_const) * grad_func(y)\n",
    "        v = v - a * grad_func(x)\n",
    "        \n",
    "        grad_h_count.append(grad_h_count[-1] + 2.2)\n",
    "        grad_g_count.append(grad_g_count[-1] + 2.2)\n",
    "        \n",
    "        if time_scale:\n",
    "            times.append(time.time())\n",
    "        \n",
    "        funcs.append(func(x))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f(x))\n",
    "        \n",
    "    if return_f:\n",
    "        return x, np.array(funcs), grad_h_count, grad_g_count, times\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta_times, funcs_times, grad_h_count_times, grad_g_count_times, times = nesterov(\n",
    "    f,\n",
    "    grad_f_stoch,\n",
    "    x_0,\n",
    "    A,\n",
    "    T=4000,\n",
    "    L_add=0,\n",
    "    restart=None,\n",
    "    time_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fgm_times, funcs_fgm_times, grad_h_count_fgm_times, grad_g_count_fgm_times, fgm_times = fgm(\n",
    "    f,\n",
    "    grad_f,\n",
    "    x_0,\n",
    "    T=1000,\n",
    "    L_const=L*370,\n",
    "    time_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ms_times, funcs_ms_times, grad_h_count_ms_times, grad_g_count_ms_times, ms_times = ms_acc_prox_method(\n",
    "    x_0, \n",
    "    L, Lh, \n",
    "    A,\n",
    "    T=200, \n",
    "    time_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_taylor1_times, funcs_taylor1_times, grad_h_count_taylor1_times, grad_g_count_taylor1_times, taylor1_times = taylor_acc_prox_method(\n",
    "    x_0, \n",
    "    L*20, Lh, \n",
    "    A,\n",
    "    T_inner=64,\n",
    "    T=50, \n",
    "    time_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_taylor2_times, funcs_taylor2_times, grad_h_count_taylor2_times, grad_g_count_taylor2_times, taylor2_times = taylor_acc_prox_method(\n",
    "    x_0, \n",
    "    L*25, Lh, \n",
    "    A,\n",
    "    T_inner=8,\n",
    "    T=700, \n",
    "    time_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_taylor3_times, funcs_taylor3_times, grad_h_count_taylor3_times, grad_g_count_taylor3_times, taylor3_times = taylor_acc_prox_method(\n",
    "    x_0, \n",
    "    L*27, Lh, \n",
    "    A,\n",
    "    T_inner=3,\n",
    "    T=700, \n",
    "    time_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt, _, __, ___, ____ = nesterov(\n",
    "    f,\n",
    "    grad_f_stoch,\n",
    "    x_0,\n",
    "    A,\n",
    "    T=18000,\n",
    "    L_add=0,\n",
    "    restart=None,\n",
    "    time_scale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_opt = funcs_fgm_times[-1] - 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = min(\n",
    "    funcs_ms_times.shape[0],\n",
    "    funcs_times.shape[0],\n",
    "    funcs_fgm_times.shape[0],\n",
    "    funcs_taylor2_times.shape[0],\n",
    "    funcs_taylor3_times.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_grad_plot = min(\n",
    "    funcs_ms_times.shape[0],\n",
    "    funcs_times.shape[0],\n",
    "    funcs_fgm_times.shape[0],\n",
    "    funcs_taylor2_times.shape[0],\n",
    "    funcs_taylor3_times.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_h_limit = int(min(\n",
    "    grad_h_count_ms_times[-1],\n",
    "    grad_h_count_times[-1],\n",
    "    grad_h_count_fgm_times[-1],\n",
    "    grad_h_count_taylor2_times[-1],\n",
    "    grad_h_count_taylor3_times[-1]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_grads_h_min_idx = np.searchsorted(grad_h_count_ms_times, grads_h_limit, side=\"right\")\n",
    "grads_h_min_idx = np.searchsorted(grad_h_count_times, grads_h_limit, side=\"right\")\n",
    "fgm_grads_h_min_idx = np.searchsorted(grad_h_count_fgm_times, grads_h_limit, side=\"right\")\n",
    "taylor2_grads_h_min_idx = np.searchsorted(grad_h_count_taylor2_times, grads_h_limit, side=\"right\")\n",
    "taylor3_grads_h_min_idx = np.searchsorted(grad_h_count_taylor3_times, grads_h_limit, side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_g_limit = int(min(\n",
    "    grad_g_count_ms_times[-1],\n",
    "    grad_g_count_times[-1],\n",
    "    grad_g_count_fgm_times[-1],\n",
    "    grad_g_count_taylor2_times[-1],\n",
    "    grad_g_count_taylor3_times[-1]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_grads_g_min_idx = np.searchsorted(grad_g_count_ms_times, grads_g_limit, side=\"right\")\n",
    "grads_g_min_idx = np.searchsorted(grad_g_count_times, grads_g_limit, side=\"right\")\n",
    "fgm_grads_g_min_idx = np.searchsorted(grad_g_count_fgm_times, grads_g_limit, side=\"right\")\n",
    "taylor2_grads_g_min_idx = np.searchsorted(grad_g_count_taylor2_times, grads_g_limit, side=\"right\")\n",
    "taylor3_grads_g_min_idx = np.searchsorted(grad_g_count_taylor3_times, grads_g_limit, side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "\n",
    "fig = plt.figure(figsize=(13,4))\n",
    "fig.tight_layout()\n",
    "ax = fig.add_subplot(131, projection='3d')\n",
    "\n",
    "ax.plot(\n",
    "    grad_h_count_ms_times[:length_grad_plot], \n",
    "    grad_g_count_ms_times[:length_grad_plot], \n",
    "    np.log10((np.minimum.accumulate(funcs_ms_times)[:length_grad_plot] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"MS accelerated FCD\"\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    grad_h_count_taylor2_times[:length_grad_plot], \n",
    "    grad_g_count_taylor2_times[:length_grad_plot], \n",
    "    np.log10((np.minimum.accumulate(funcs_taylor2_times)[:length_grad_plot] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"Taylor accelerated FCD (k=8)\"\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    grad_h_count_taylor3_times[:length_grad_plot], \n",
    "    grad_g_count_taylor3_times[:length_grad_plot], \n",
    "    np.log10((np.minimum.accumulate(funcs_taylor3_times)[:length_grad_plot] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"Taylor accelerated FCD (k=3)\"\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    grad_h_count_times[:length_grad_plot], \n",
    "    grad_g_count_times[:length_grad_plot], \n",
    "    np.log10((np.minimum.accumulate(funcs_times)[:length_grad_plot] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"FCD\"\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    grad_h_count_fgm_times[:length_grad_plot], \n",
    "    grad_g_count_fgm_times[:length_grad_plot], \n",
    "    np.log10((np.minimum.accumulate(funcs_fgm_times)[:length_grad_plot] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"FGM\"\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$\\\\nabla h_i$ calculations\")\n",
    "ax.set_ylabel(\"$\\\\nabla g_i$ calculations\")\n",
    "ax.set_zlabel(\"$\\\\log_{10}$ function value\")\n",
    "ax.view_init(azim=-38)\n",
    "\n",
    "ax1 = fig.add_subplot(132)\n",
    "ax1.plot(\n",
    "    grad_h_count_ms_times[:ms_grads_h_min_idx], \n",
    "    np.log10((np.minimum.accumulate(funcs_ms_times)[:ms_grads_h_min_idx] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"MS accelerated FCD\"\n",
    ")\n",
    "\n",
    "ax1.plot(\n",
    "    grad_h_count_taylor2_times[:taylor2_grads_h_min_idx], \n",
    "    np.log10((np.minimum.accumulate(funcs_taylor2_times)[:taylor2_grads_h_min_idx] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"Taylor accelerated FCD (k=8)\"\n",
    ")\n",
    "\n",
    "ax1.plot(\n",
    "    grad_h_count_taylor3_times[:taylor3_grads_h_min_idx], \n",
    "    np.log10((np.minimum.accumulate(funcs_taylor3_times)[:taylor3_grads_h_min_idx] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"Taylor accelerated FCD (k=3)\"\n",
    ")\n",
    "\n",
    "ax1.plot(\n",
    "    grad_h_count_times[:grads_h_min_idx], \n",
    "    np.log10((np.minimum.accumulate(funcs_times)[:grads_h_min_idx] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"FCD\"\n",
    ")\n",
    "\n",
    "ax1.plot(\n",
    "    grad_h_count_fgm_times[:fgm_grads_h_min_idx],\n",
    "    np.log10((np.minimum.accumulate(funcs_fgm_times)[:fgm_grads_h_min_idx] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"FGM\"\n",
    ")\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_xlabel(\"$\\\\nabla h_i$ calculations\")\n",
    "ax1.set_ylabel(\"$\\\\log_{10}$ function value\")\n",
    "\n",
    "ax2 = fig.add_subplot(133)\n",
    "ax2.plot(\n",
    "    grad_g_count_ms_times[:ms_grads_g_min_idx], \n",
    "    np.log10((np.minimum.accumulate(funcs_ms_times)[:ms_grads_g_min_idx] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"MS accelerated FCD\"\n",
    ")\n",
    "\n",
    "ax2.plot(\n",
    "    grad_g_count_taylor2_times[:taylor2_grads_g_min_idx], \n",
    "    np.log10((np.minimum.accumulate(funcs_taylor2_times)[:taylor2_grads_g_min_idx] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"Taylor accelerated FCD (k=8)\"\n",
    ")\n",
    "\n",
    "ax2.plot(\n",
    "    grad_g_count_taylor3_times[:taylor3_grads_g_min_idx], \n",
    "    np.log10((np.minimum.accumulate(funcs_taylor3_times)[:taylor3_grads_g_min_idx] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"Taylor accelerated FCD (k=3)\"\n",
    ")\n",
    "\n",
    "ax2.plot(\n",
    "    grad_g_count_times[:grads_g_min_idx], \n",
    "    np.log10((np.minimum.accumulate(funcs_times[:grads_g_min_idx]) - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"FCD\"\n",
    ")\n",
    "\n",
    "ax2.plot(\n",
    "    grad_g_count_fgm_times[:fgm_grads_g_min_idx],\n",
    "    np.log10((np.minimum.accumulate(funcs_fgm_times)[:fgm_grads_g_min_idx] - func_opt) / (f(x_0) - func_opt)),\n",
    "    label=\"FGM\"\n",
    ")\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_xlabel(\"$\\\\nabla g_i$ calculations\")\n",
    "ax2.set_ylabel(\"$\\\\log_{10}$ function value\")\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "plt.savefig(\"ms_taylor_3d.pdf\", dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get working time limit for all methods\n",
    "time_max = min(\n",
    "    ms_times[-1] - ms_times[0],\n",
    "    times[-1] - times[0],\n",
    "    fgm_times[-1] - fgm_times[0],\n",
    "    taylor2_times[-1] - taylor2_times[0],\n",
    "    taylor3_times[-1] - taylor3_times[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of iteration, executing at the time `time_max`\n",
    "ms_times_max_idx = np.searchsorted(np.array(ms_times) - ms_times[0], time_max, side=\"right\")\n",
    "times_max_idx = np.searchsorted(np.array(times) - times[0], time_max, side=\"right\")\n",
    "fgm_times_max_idx = np.searchsorted(np.array(fgm_times) - fgm_times[0], time_max, side=\"right\")\n",
    "taylor2_times_max_idx = np.searchsorted(np.array(taylor2_times) - taylor2_times[0], time_max, side=\"right\")\n",
    "taylor3_times_max_idx = np.searchsorted(np.array(taylor3_times) - taylor3_times[0], time_max, side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy((np.array(ms_times) - ms_times[0])[:ms_times_max_idx], (funcs_ms_times - func_opt)[:ms_times_max_idx] / (f(x_0) - func_opt), \n",
    "             label=\"MS accelerated FCD\")\n",
    "plt.semilogy((np.array(taylor2_times) - taylor2_times[0])[:taylor2_times_max_idx], (funcs_taylor2_times - func_opt)[:taylor2_times_max_idx] / (f(x_0) - func_opt), \n",
    "             label=\"Taylor accelerated FCD (k=8)\")\n",
    "plt.semilogy((np.array(taylor3_times) - taylor3_times[0])[:taylor3_times_max_idx], (funcs_taylor3_times - func_opt)[:taylor3_times_max_idx] / (f(x_0) - func_opt), \n",
    "             label=\"Taylor accelerated FCD (k=3)\")\n",
    "plt.semilogy((np.array(times) - times[0])[:times_max_idx], (funcs_times - func_opt)[:times_max_idx] / (f(x_0) - func_opt), label=\"FCD\")\n",
    "plt.semilogy((np.array(fgm_times) - fgm_times[0])[:fgm_times_max_idx], (funcs_fgm_times[:fgm_times_max_idx] - func_opt) / (f(x_0) - func_opt), label=\"FGM\")\n",
    "\n",
    "plt.xlabel(r\"Time, $T$ (s)\", fontsize=12)\n",
    "plt.ylabel(r\"Function value (log-scale)\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"ms_taylor_time.pdf\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of iteration, executing at the time 0.5 s\n",
    "time_min = 7.0\n",
    "\n",
    "ms_times_min_idx = np.searchsorted(np.array(ms_times) - ms_times[0], time_min, side=\"right\")\n",
    "times_min_idx = np.searchsorted(np.array(times) - times[0], time_min, side=\"right\")\n",
    "fgm_times_min_idx = np.searchsorted(np.array(fgm_times) - fgm_times[0], time_min, side=\"right\")\n",
    "taylor2_times_min_idx = np.searchsorted(np.array(taylor2_times) - taylor2_times[0], time_min, side=\"right\")\n",
    "taylor3_times_min_idx = np.searchsorted(np.array(taylor3_times) - taylor3_times[0], time_min, side=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy((np.array(ms_times) - ms_times[0])[ms_times_min_idx:ms_times_max_idx], (funcs_ms_times - func_opt)[ms_times_min_idx:ms_times_max_idx] / (f(x_0) - func_opt), \n",
    "             label=\"MS accelerated FCD\")\n",
    "plt.semilogy((np.array(taylor2_times) - taylor2_times[0])[taylor2_times_min_idx:taylor2_times_max_idx], (funcs_taylor2_times - func_opt)[taylor2_times_min_idx:taylor2_times_max_idx] / (f(x_0) - func_opt), \n",
    "             label=\"Taylor accelerated FCD (k=8)\")\n",
    "plt.semilogy((np.array(taylor3_times) - taylor3_times[0])[taylor3_times_min_idx:taylor3_times_max_idx], (funcs_taylor3_times - func_opt)[taylor3_times_min_idx:taylor3_times_max_idx] / (f(x_0) - func_opt), \n",
    "             label=\"Taylor accelerated FCD (k=3)\")\n",
    "plt.semilogy((np.array(times) - times[0])[times_min_idx:times_max_idx], (funcs_times - func_opt)[times_min_idx:times_max_idx] / (f(x_0) - func_opt), label=\"FCD\")\n",
    "plt.semilogy((np.array(fgm_times) - fgm_times[0])[fgm_times_min_idx:fgm_times_max_idx], (funcs_fgm_times[fgm_times_min_idx:fgm_times_max_idx] - func_opt) / (f(x_0) - func_opt), label=\"FGM\")\n",
    "\n",
    "\n",
    "plt.xlabel(r\"Time, $T$ (s)\", fontsize=12)\n",
    "plt.ylabel(r\"Function value (log-scale)\", fontsize=12)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(\"ms_taylor_time_detailed.pdf\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get iterations limit for all methods\n",
    "length = min(\n",
    "    funcs_ms_times.shape[0],\n",
    "    funcs_times.shape[0],\n",
    "    funcs_fgm_times.shape[0],\n",
    "    funcs_taylor1_times.shape[0],\n",
    "    funcs_taylor2_times.shape[0],\n",
    "    funcs_taylor3_times.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy((funcs_ms_times[:length] - func_opt) / (f(x_0) - func_opt), \n",
    "             label=\"MS accelerated FCD\")\n",
    "plt.semilogy((funcs_taylor2_times[:length] - func_opt) / (f(x_0) - func_opt), \n",
    "             label=\"Taylor accelerated FCD (k=8)\")\n",
    "plt.semilogy((funcs_taylor3_times[:length] - func_opt) / (f(x_0) - func_opt), \n",
    "             label=\"Taylor accelerated FCD (k=3)\")\n",
    "plt.semilogy((funcs_times[:length] - func_opt) / (f(x_0) - func_opt), label=\"FCD\")\n",
    "plt.semilogy((funcs_fgm_times[:length] - func_opt) / (f(x_0) - func_opt), label=\"FGM\")\n",
    "\n",
    "plt.xlabel(r\"Number of iterations, $N$\", fontsize=12)\n",
    "plt.ylabel(r\"Function value (log-scale)\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"ms_taylor_iters.pdf\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
